{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from cvxopt import matrix, solvers\n",
    "from numpy import linalg\n",
    "import cvxopt\n",
    "import cvxopt.solvers\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score,confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SVM(object):\n",
    "\n",
    "    def __init__(self, kernel=linear_kernel, C=None):\n",
    "        self.kernel = kernel\n",
    "        self.C = C\n",
    "        if self.C is not None: \n",
    "            self.C = float(self.C)\n",
    "\n",
    "\n",
    "    def set_c (self,C):\n",
    "        self.C = C\n",
    "        if self.C is not None: \n",
    "            self.C = float(self.C)\n",
    "    def set_kernel (self,kernel):\n",
    "        self.kernel = kernel\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "\n",
    "        # Gram matrix\n",
    "        K = np.zeros((n_samples, n_samples))\n",
    "        for i in range(n_samples):\n",
    "            for j in range(n_samples):\n",
    "                K[i,j] = self.kernel(X[i], X[j])\n",
    "\n",
    "        P = cvxopt.matrix(np.outer(y,y) * K)\n",
    "        q = cvxopt.matrix(np.ones(n_samples) * -1)\n",
    "        A = cvxopt.matrix(y, (1,n_samples))\n",
    "        b = cvxopt.matrix(0.0)\n",
    "\n",
    "        if self.C is None:\n",
    "            G = cvxopt.matrix(np.diag(np.ones(n_samples) * -1))\n",
    "            h = cvxopt.matrix(np.zeros(n_samples))\n",
    "        else:\n",
    "            tmp1 = np.diag(np.ones(n_samples) * -1)\n",
    "            tmp2 = np.identity(n_samples)\n",
    "            G = cvxopt.matrix(np.vstack((tmp1, tmp2)))\n",
    "            tmp1 = np.zeros(n_samples)\n",
    "            tmp2 = np.ones(n_samples) * self.C\n",
    "            h = cvxopt.matrix(np.hstack((tmp1, tmp2)))\n",
    "\n",
    "        # solve QP problem\n",
    "        solution = cvxopt.solvers.qp(P, q, G, h, A, b)\n",
    "\n",
    "        # Lagrange multipliers\n",
    "        a = np.ravel(solution['x'])\n",
    "\n",
    "        # Support vectors have non zero lagrange multipliers\n",
    "        sv = a > 1e-5\n",
    "        ind = np.arange(len(a))[sv]\n",
    "        self.__a = a[sv]\n",
    "        self.__sv = X[sv]\n",
    "        self.__sv_y = y[sv]\n",
    "        print(\"%d support vectors out of %d points\" % (len(self.__a), n_samples))\n",
    "\n",
    "        # Intercept\n",
    "        self.b = 0\n",
    "        for n in range(len(self.__a)):\n",
    "            self.b += self.__sv_y[n]\n",
    "            self.b -= np.sum(self.__a * self.__sv_y * K[ind[n],sv])\n",
    "        self.b /= len(self.__a)\n",
    "\n",
    "        # Weight vector\n",
    "        if self.kernel == linear_kernel:\n",
    "            self.w = np.zeros(n_features)\n",
    "            for n in range(len(self.__a)):\n",
    "                self.w += self.__a[n] * self.__sv_y[n] * self.__sv[n]\n",
    "        else:\n",
    "            self.w = None\n",
    "\n",
    "    def ــproject(self, X):\n",
    "        if self.w is not None:\n",
    "            return np.dot(X, self.w) + self.b\n",
    "        else:\n",
    "            y_predict = np.zeros(len(X))\n",
    "            for i in range(len(X)):\n",
    "                s = 0\n",
    "                for a, sv_y, sv in zip(self.__a, self.__sv_y, self.__sv):\n",
    "                    s += a * sv_y * self.kernel(X[i], sv)\n",
    "                y_predict[i] = s\n",
    "            return y_predict + self.b\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.sign(self.ــproject(X))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def linear_kernel(x1, x2):\n",
    "    return np.dot(x1, x2)\n",
    "\n",
    "def polynomial_kernel(x, y, p=2):\n",
    "    return (1 + np.dot(x, y)) ** p\n",
    "\n",
    "def gaussian_kernel(x, y, sigma=5.0):\n",
    "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train0 = np.array(pd.read_csv('train0.csv'))\n",
    "train8 = np.array(pd.read_csv('train8.csv'))\n",
    "train1 = np.array(pd.read_csv('train1.csv'))\n",
    "train2 = np.array(pd.read_csv('train2.csv'))\n",
    "X_train = []\n",
    "X_train.append(train0)\n",
    "X_train.append(train1)\n",
    "X_train.append(train2)\n",
    "X_train.append(train8)\n",
    "\n",
    "# train_labels = np.concatenate((np.ones(train0.shape[0])*-1, (np.ones(train8.shape[0]))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "test0 = pd.read_csv('test0.csv')\n",
    "test8 = pd.read_csv('test8.csv')\n",
    "test1 = pd.read_csv('test1.csv')\n",
    "test2 = pd.read_csv('test2.csv')\n",
    "X_test = []\n",
    "X_test.append(test0)\n",
    "X_test.append(test1)\n",
    "X_test.append(test2)\n",
    "X_test.append(test3)\n",
    "\n",
    "# test_x = np.concatenate((test0,test8),axis = 0)\n",
    "# test_labels = np.concatenate((np.ones(test0.shape[0])*-1 , (np.ones(test8.shape[0]))), axis=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "del test0,test8,train0,train8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_x/256\n",
    "test_x = test_x/256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = shuffle(train_x, train_labels, random_state=0)\n",
    "X_test, y_test = shuffle(test_x, test_labels, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, a, y_train, b = train_test_split(train_x, train_labels, test_size=0.8, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -1.6092e+02 -5.0002e+02  1e+04  9e+01  3e+00\n",
      " 1: -2.7509e+02 -4.0616e+02  6e+03  3e+01  1e+00\n",
      " 2: -6.5023e+01 -6.4370e+01  1e+03  6e+00  2e-01\n",
      " 3: -1.5506e+01 -2.3432e+01  3e+02  1e+00  4e-02\n",
      " 4: -4.7927e+00 -1.2778e+01  7e+01  3e-01  1e-02\n",
      " 5: -2.1500e+00 -9.6096e+00  3e+01  1e-01  3e-03\n",
      " 6: -2.6882e+00 -6.5705e+00  7e+00  2e-02  7e-04\n",
      " 7: -3.4906e+00 -5.5344e+00  3e+00  4e-03  1e-04\n",
      " 8: -4.1817e+00 -5.2751e+00  1e+00  4e-16  5e-14\n",
      " 9: -4.7065e+00 -5.0868e+00  4e-01  4e-16  5e-14\n",
      "10: -4.9161e+00 -5.0318e+00  1e-01  4e-16  5e-14\n",
      "11: -5.0075e+00 -5.0207e+00  1e-02  3e-16  6e-14\n",
      "12: -5.0194e+00 -5.0197e+00  2e-04  7e-16  6e-14\n",
      "13: -5.0196e+00 -5.0196e+00  3e-06  3e-16  6e-14\n",
      "Optimal solution found.\n",
      "125 support vectors out of 2354 points\n",
      "1932 out of 1952 predictions correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9897540983606558"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVM()\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(test_x)\n",
    "correct = np.sum(y_predict == test_labels)\n",
    "print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "y_predict = clf.predict(test_x)\n",
    "# confusion_matrix(y_predict,test_labels)\n",
    "accuracy_score(y_predict,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -8.9733e+01 -4.9077e+02  2e+04  3e+01  3e-13\n",
      " 1: -1.6046e+01 -4.5008e+02  1e+03  1e+00  3e-13\n",
      " 2: -9.4998e+00 -1.7107e+02  3e+02  2e-01  6e-14\n",
      " 3: -6.0286e+00 -8.8085e+01  1e+02  9e-02  3e-14\n",
      " 4: -3.7287e+00 -3.9110e+01  5e+01  3e-02  2e-14\n",
      " 5: -2.5182e+00 -1.7679e+01  2e+01  1e-02  2e-14\n",
      " 6: -2.5923e+00 -8.2629e+00  7e+00  3e-03  2e-14\n",
      " 7: -2.8186e+00 -4.4326e+00  2e+00  5e-16  2e-14\n",
      " 8: -3.0719e+00 -3.9588e+00  9e-01  2e-16  2e-14\n",
      " 9: -3.2407e+00 -3.6367e+00  4e-01  5e-16  2e-14\n",
      "10: -3.3346e+00 -3.4535e+00  1e-01  2e-16  2e-14\n",
      "11: -3.3698e+00 -3.4052e+00  4e-02  9e-16  2e-14\n",
      "12: -3.3857e+00 -3.3868e+00  1e-03  9e-16  2e-14\n",
      "13: -3.3862e+00 -3.3862e+00  2e-05  2e-16  2e-14\n",
      "14: -3.3862e+00 -3.3862e+00  2e-07  2e-16  2e-14\n",
      "Optimal solution found.\n",
      "124 support vectors out of 2354 points\n",
      "1937 out of 1952 predictions correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9923155737704918"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVM(C = 0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(test_x)\n",
    "correct = np.sum(y_predict == test_labels)\n",
    "print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "y_predict = clf.predict(test_x)\n",
    "# confusion_matrix(y_predict,test_labels)\n",
    "accuracy_score(y_predict,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -5.6046e+02 -5.4737e+02  8e+03  2e+01  1e-15\n",
      " 1: -2.5119e+02 -4.3206e+02  5e+02  7e-01  1e-15\n",
      " 2: -2.0184e+02 -2.6080e+02  6e+01  3e-15  2e-15\n",
      " 3: -2.1217e+02 -2.2057e+02  8e+00  2e-15  2e-15\n",
      " 4: -2.1209e+02 -2.1683e+02  5e+00  4e-15  1e-15\n",
      " 5: -2.1192e+02 -2.1638e+02  4e+00  4e-15  1e-15\n",
      " 6: -2.1228e+02 -2.1406e+02  2e+00  4e-16  1e-15\n",
      " 7: -2.1261e+02 -2.1319e+02  6e-01  7e-15  1e-15\n",
      " 8: -2.1279e+02 -2.1290e+02  1e-01  7e-15  2e-15\n",
      " 9: -2.1283e+02 -2.1284e+02  1e-02  3e-15  2e-15\n",
      "10: -2.1284e+02 -2.1284e+02  3e-04  4e-16  2e-15\n",
      "11: -2.1284e+02 -2.1284e+02  4e-06  2e-15  2e-15\n",
      "Optimal solution found.\n",
      "2350 support vectors out of 2354 points\n",
      "1652 out of 1952 predictions correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.8463114754098361"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gaussian_kernel(x, y, sigma= 2.0):\n",
    "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
    "\n",
    "clf = SVM(kernel = gaussian_kernel, C=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(test_x)\n",
    "correct = np.sum(y_predict == test_labels)\n",
    "print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "y_predict = clf.predict(test_x)\n",
    "# confusion_matrix(y_predict,test_labels)\n",
    "accuracy_score(y_predict,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -9.1288e+01 -5.0600e+02  2e+04  3e+01  3e-15\n",
      " 1: -3.6075e+01 -4.6719e+02  1e+03  1e+00  3e-15\n",
      " 2: -2.1870e+01 -1.6932e+02  1e+02  4e-15  2e-15\n",
      " 3: -3.0465e+01 -6.4597e+01  3e+01  7e-16  2e-15\n",
      " 4: -3.4931e+01 -4.9218e+01  1e+01  2e-16  2e-15\n",
      " 5: -3.6656e+01 -4.4694e+01  8e+00  2e-15  2e-15\n",
      " 6: -3.7944e+01 -4.1598e+01  4e+00  2e-16  2e-15\n",
      " 7: -3.8650e+01 -4.0117e+01  1e+00  6e-16  2e-15\n",
      " 8: -3.9010e+01 -3.9443e+01  4e-01  2e-16  2e-15\n",
      " 9: -3.9153e+01 -3.9209e+01  6e-02  5e-16  2e-15\n",
      "10: -3.9176e+01 -3.9178e+01  2e-03  2e-15  2e-15\n",
      "11: -3.9177e+01 -3.9177e+01  5e-05  3e-16  2e-15\n",
      "12: -3.9177e+01 -3.9177e+01  9e-07  2e-15  2e-15\n",
      "Optimal solution found.\n",
      "616 support vectors out of 2354 points\n",
      "1936 out of 1952 predictions correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9918032786885246"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def gaussian_kernel(x, y, sigma= 10.0):\n",
    "    return np.exp(-linalg.norm(x-y)**2 / (2 * (sigma ** 2)))\n",
    "\n",
    "clf = SVM(kernel = gaussian_kernel, C=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(test_x)\n",
    "correct = np.sum(y_predict == test_labels)\n",
    "print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "y_predict = clf.predict(test_x)\n",
    "# confusion_matrix(y_predict,test_labels)\n",
    "accuracy_score(y_predict,test_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     pcost       dcost       gap    pres   dres\n",
      " 0: -6.7477e-02 -2.3585e+02  5e+03  1e+01  6e-14\n",
      " 1: -6.3526e-02 -1.9553e+02  2e+02  2e-01  9e-14\n",
      " 2: -3.5599e-02 -2.5714e+01  3e+01  2e-02  9e-14\n",
      " 3: -3.6711e-02 -5.0704e+00  6e+00  3e-03  7e-14\n",
      " 4: -3.0102e-02 -1.8992e+00  2e+00  1e-03  3e-14\n",
      " 5: -2.3610e-02 -1.0317e+00  1e+00  6e-04  2e-14\n",
      " 6: -1.3822e-02 -3.2390e-01  4e-01  2e-04  8e-15\n",
      " 7: -1.1167e-02 -6.8246e-02  7e-02  2e-05  6e-15\n",
      " 8: -1.2075e-02 -2.3446e-02  1e-02  3e-06  6e-15\n",
      " 9: -1.2826e-02 -1.6760e-02  4e-03  2e-16  6e-15\n",
      "10: -1.3854e-02 -1.4832e-02  1e-03  1e-16  6e-15\n",
      "11: -1.4165e-02 -1.4325e-02  2e-04  2e-16  6e-15\n",
      "12: -1.4226e-02 -1.4236e-02  1e-05  2e-16  6e-15\n",
      "13: -1.4230e-02 -1.4230e-02  2e-07  2e-16  6e-15\n",
      "14: -1.4230e-02 -1.4230e-02  3e-09  2e-16  6e-15\n",
      "Optimal solution found.\n",
      "171 support vectors out of 2354 points\n",
      "1942 out of 1952 predictions correct\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9948770491803278"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = SVM(kernel = polynomial_kernel ,C=0.1)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "y_predict = clf.predict(test_x)\n",
    "correct = np.sum(y_predict == test_labels)\n",
    "print(\"%d out of %d predictions correct\" % (correct, len(y_predict)))\n",
    "y_predict = clf.predict(test_x)\n",
    "# confusion_matrix(y_predict,test_labels)\n",
    "accuracy_score(y_predict,test_labels)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
